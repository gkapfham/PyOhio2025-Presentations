---
title: "ExecExam: A Tool to Facilitate Effective Executable Examinations in Python"
date: "2025-05-15"
date-format: long
author: "Pallas-Athena Cain, Hemani Alaparthi, and Gregory Kapfhammer"
format: 
  revealjs:
    theme: default
    slide-number: false
    incremental: false
    code-fold: true
    code-tools: true
    code-link: true
    history: false
    scrollable: true
    transition: slide
    highlight-style: github
    footer: "PyCon Education Summit 2025"
    css: ./styles.css
---

## What is an Executable Examination?

::: {.fragment style="margin-top: -0.15em; font-size: 0.8em;"}

{{< iconify fa6-solid gears >}} **Goal: Assess a student's ability to program
with real tools**

  - A student writes, modifies, and runs code to solve a real problem
  - Graded via automated tests that use Pytest tests and assertions
  - Unlike static examinations an executable examination assesses:
    - Programming logic
    - Debugging ability
    - Tool use (e.g., text editor, terminal, IDE, and Git)

:::

::: {.fragment .fade style="margin-top: -0.15em; font-size: 0.8em;"}

üéØ **Like a take-home project ‚Äî but precise, consistent, and scalable!**

**Reference**: Chris Bourke, Yael Erez, and Orit Hazzan. 2023. "Executable Exams: Taxonomy,
Implementation and Prospects". In Proceedings of 54th SIGCSE.

:::

## Problems with Computing Assessments

::: fragment

{{< iconify game-icons team-idea >}} **Why do we need better assessments?**

- Manual grading is slow and inconsistent
- Students often don‚Äôt know why their code fails
- Feedback is shallow or missing altogether
- Limited assessment of effective tool use
- Pytest not a good fit for assessment

:::

::: {.fragment .fade style="margin-top: -0.15em; font-size: 0.9em;"}

üö´ ‚ÄúTest failed‚Äù isn't enough! ExecExam presents a compelling alternative to
either manual assessment or running only Pytest.

:::

## What is ExecExam?

::: fragment

{{< iconify fa6-solid gears >}} **Scalable, feedback-rich assessment tool built in Python**

:::: {.columns}

::: {.column width="65%"}

- Runs Pytest tests on student code
- Reports all test failures and context
- Clearly explains why a test failed
- Suggests how to fix tested function
- Uses LLMs for enhanced feedback
:::

::: {.column width="35%" .middle}
![](ExecExam_-_Logo_-_300.png){width="100%"}
:::

:::

::: {.fragment .fade style="margin-top: -0.75em; font-size: 0.9em;"}

{{< iconify fa6-solid lightbulb >}} Let's learn more about ExecExam's features
and how you can integrate them into your assessments for your own courses!

:::

::::

## Understanding ExecExam's Output

![](terminal.png)

## Key Features

**Why use ExecExam?**

- üß™ Full Pytest coverage, Streamlines assessment
- üíª Can be run throughout the student coding process
- üß† AI-powered failure analysis
- ‚öôÔ∏è GitHub integration, easy CI/CD
- üîÅ Enhance the learning experience by offering actionable insights throughout the coding process

## For Instructors

**How to adopt it**

- Design scaffolded coding tasks
- Write tests
- Deploy with GitHub Actions or CLI
- Grade fairly, at scale

üß∞ Lightweight setup‚Äîjust Python, Git, and your test cases

## Future Work

::: {.fragment style="margin-top: -0.15em; font-size: 0.7em;"}

**What‚Äôs next for ExecExam?**

- üìä Analytics & Instructor Features
  - Store test outcomes and feedback over time
  - Visualize student debugging and improvement paths
  - Log LLM interactions to evaluate effectiveness
  - Hold out hidden test cases for instructor-only grading

- üß† Adaptive Feedback Loops
  - Tailor feedback complexity to student performance
  - Allow students to rate LLM feedback

:::

## Key Takeaways

::: {.fragment .fade .tight-boxed-content style="margin-top: -0.15em; font-size: 0.7em;"}

**Better exam grading, better learning**

- Helps students debug and learn
- Saves teachers time
- More fair + consistent grading
- Scales to large classes

:::

::: {.fragment .fade .tight-boxed-content style="margin-top: -0.15em; font-size: 0.6em;"}

**üöÄ Try ExecExam! Let‚Äôs build smarter CS assessments!**

- üîó GitHub Repository: [https://github.com/GatorEducator/execexam](https://github.com/GatorEducator/execexam)
- üíª PyPI: [https://pypi.org/project/execexam/](https://pypi.org/project/execexam/)
- ü§ù Reach out or contribute 
  - Pallas-Athena Cain: **cain01@allegheny.edu** 
  - Gregory M. Kapfhammer: **gkapfham@allegheny.edu**

ü¶ö Consider a birds of a feather session about automated grading

:::

## References

::: {.fragment .tight-boxed-content style="margin-top: -0.15em; font-size: 0.5em;"}

- Jo√£o Paulo Barros, Lu√≠s Estevens, Rui Dias, Rui Pais, and Elisabete Soeiro.
2003. Using lab exams to ensure programming practice in an introductory
programming course. SIGCSE Bull. 35, 3 (September 2003), 16‚Äì20.
- Jonathan Corley, Ana Stanescu, Lewis Baumstark, and Michael C. Orsega. 2020.
Paper Or IDE? The Impact of Exam Format on Student Performance in a CS1 Course.
In Proceedings of the 51st ACM Technical Symposium on Computer Science
Education (SIGCSE '20). Association for Computing Machinery, New York, NY, USA,
706‚Äì712.
- Scott Grissom, Laurie Murphy, Ren√©e McCauley, and Sue Fitzgerald. 2016. Paper
vs. Computer-based Exams: A Study of Errors in Recursive Binary Tree
Algorithms. In Proceedings of the 47th ACM Technical Symposium on Computing
Science Education (SIGCSE '16). Association for Computing Machinery, New York,
NY, USA, 6‚Äì11.
- Sevkli, Z. 2024. Assessing the Impact of Open-Resource Access on Student
Performance in Computer-Based Examinations. In Proceedings of the 2024 ASEE
Annual Conference & Exposition, Portland, Oregon.
- Vesa Lappalainen, Antti-Jussi Lakanen, and Harri H√∂gmander. 2016. Paper-based
vs computer-based exams in CS1. In Proceedings of the 16th Koli Calling
International Conference on Computing Education Research (Koli Calling '16).
Association for Computing Machinery, New York, NY, USA, 172‚Äì173.

:::
